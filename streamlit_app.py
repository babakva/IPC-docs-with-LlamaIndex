import streamlit as st
from llama_index.core import VectorStoreIndex, ServiceContext, Document
from llama_index.llms.openai import OpenAI
import openai
from llama_index.core import SimpleDirectoryReader

st.set_page_config(page_title="Chat with International Patent Classifications 2024.01 Section C - Chemistry; Metallurgy docs.", page_icon="", layout="centered", initial_sidebar_state="auto", menu_items=None)
openai.api_key = st.secrets.openai_key
st.title("International Patent Classifications 2024.01 Section C - Chemistry; Metallurgy docs")
st.info("IPC data can be found in this [link](https://www.wipo.int/classifications/ipc/en/ITsupport/Version20240101/) ", icon="ðŸ“ƒ")
         
if "messages" not in st.session_state.keys(): # Initialize the chat messages history
    st.session_state.messages = [
        {"role": "assistant", "content": "Ask me a question about the classifications or choose one from below. \n 1. What are the latest advancements in active carbon technology for filtration purposes? \n 2. Are there any patents related to the use of ion-exchange materials in sugar juice processing? \n 3. How can physical means such as osmosis or filtration be used to separate impurities from sugar juices?"},
    ]

@st.cache_resource(show_spinner=False)
def load_data():
    with st.spinner(text="Loading and indexing the IPC docs â€“ hang tight! This should take 1-2 minutes."):
        reader = SimpleDirectoryReader(input_dir="./data", recursive=True)
        docs = reader.load_data()
        service_context = ServiceContext.from_defaults(llm=OpenAI(model="gpt-3.5-turbo", temperature=0.0, system_prompt="You are an expert on patent law and your job is to answer users questions. Assume that all questions are related to the Patents. Keep your answers technical and based on facts â€“ do not hallucinate features."))
        index = VectorStoreIndex.from_documents(docs, service_context=service_context)
        return index

index = load_data()

if "chat_engine" not in st.session_state.keys(): # Initialize the chat engine
        st.session_state.chat_engine = index.as_chat_engine(chat_mode="condense_question", verbose=True)

if prompt := st.chat_input("Ask me"):
    st.session_state.messages.append({"role": "user", "content": prompt})

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Searching for answer..."):
            response = st.session_state.chat_engine.chat(prompt)
            st.write(response.response)
            message = {"role": "assistant", "content": response.response}
            st.session_state.messages.append(message)